{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b-4fIdfaDIM"
   },
   "source": [
    "\n",
    "<div style=\"color:#ffffff;\n",
    "          font-size:50px;\n",
    "          font-style:italic;\n",
    "          text-align:left;\n",
    "          font-family: 'Lucida Bright';\n",
    "          background:#4686C8;\">\n",
    "  \t&nbsp; Machine Translation using Hugging Face + pretrained  + pipeline\n",
    "</div>\n",
    "<br>   \n",
    "<div style=\"\n",
    "          font-size:20px;\n",
    "          text-align:left;\n",
    "          font-family: 'Palatino';\n",
    "          \">\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project: Machine Translation<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Author: George Barrinuevo<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Date: 06/27/2025<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div style=\"color:#ffffff; \n",
    "          font-size:30px; \n",
    "          font-style:italic; \n",
    "          text-align:left;\n",
    "          font-family: 'Lucida Bright';\n",
    "          background:#4686C8;\">\n",
    "  \t      &nbsp; Project Notes\n",
    "</div>\n",
    "<div style=\"\n",
    "          font-size:16px; \n",
    "          text-align:left;\n",
    "          font-family: 'Cambria';\">\n",
    "    \n",
    "Here are my thoughts on this project.\n",
    "- <b>PROS</b>:\n",
    "    - Pretrained models are used to save time over coding a new model. Also, this avoids training the model which uses a lot of CPU/GPU/TPU processing, saving time.\n",
    "    - The pretrained model can be converted to TensorFlow/Keras format which the author prefers over PyTorch.\n",
    "    - HuggingFace has many datasets for language translation that can be used in this notebook.\n",
    "    - A pipeline method is used simplifying the coding process.\n",
    "- <b>CONS</b>:\n",
    "    - There is no customization of the model since the model is fixed. Can not use Transfer Learning methods to modify the pretrained model.\n",
    "    - Can not use other dataset sources since that would require additional preprocessing custom code so that the input text is in a format the tokenizer expects.\n",
    "- <b>INFO</b>:\n",
    "    - This model uses Transformers, a time-series or sequence model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div style=\"color:#ffffff; \n",
    "          font-size:30px; \n",
    "          font-style:italic; \n",
    "          text-align:left;\n",
    "          font-family: 'Lucida Bright';\n",
    "          background:#4686C8;\">\n",
    "  \t      &nbsp; Install Python Libraries and Load the Libraries\n",
    "</div>\n",
    "<div style=\"\n",
    "          font-size:16px; \n",
    "          text-align:left;\n",
    "          font-family: 'Cambria';\">\n",
    "After installing the first time, will have to restart the kernel and re-run this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in ./myenv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in ./myenv/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in ./myenv/lib/python3.12/site-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in ./myenv/lib/python3.12/site-packages (from sacremoses) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./myenv/lib/python3.12/site-packages (from sacremoses) (1.5.1)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.12/site-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TFAutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div style=\"color:#ffffff; \n",
    "          font-size:30px; \n",
    "          font-style:italic; \n",
    "          text-align:left;\n",
    "          font-family: 'Lucida Bright';\n",
    "          background:#4686C8;\">\n",
    "  \t      &nbsp; HuggingFace Access Token\n",
    "</div>\n",
    "<div style=\"\n",
    "          font-size:16px; \n",
    "          text-align:left;\n",
    "          font-family: 'Cambria';\">\n",
    "    \n",
    "To use the HuggingFace pretrained models, this notebook must have the HuggingFace Access Token configured.\n",
    "To created the Access Token, follow these instructions:\n",
    "- https://huggingface.co/docs/hub/en/security-tokens\n",
    "\n",
    "To configure this notebook with the Access Token, there are 3 ways to do this. Just select only one of these.\n",
    "- <b>User Prompt</b>: The user can manually enter their Access Token via the user prompt. To use this option, set use_selection to 'user_prompt'.\n",
    "- <b>Google Colab</b>: The user can configure Google Colab to store this Access Token in the secret key section. To use this option, set use_selection to 'google_colab.\n",
    "    - You can find more info here: https://pyimagesearch.com/2025/04/04/configure-your-hugging-face-access-token-in-colab-environment/\n",
    "- <b>Stored Token</b>: The Access Token can be stored on a local file. To use this option, set use_selection to 'stored_token'.\n",
    "    - Here is the basic steps to store the Access Token in a local file:<br>\n",
    "              vi ~/.cache/huggingface/stored_token<br>\n",
    "                  # Enter this info in to the file. Can use any name for the 'test-01' part. Substitue hf_* with your actual HuggingFace Access Token.<br>\n",
    "                  [test-01]<br>\n",
    "                  hf_token = hf_*<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_selection = 'stored_token'    # Values: 'user_prompt', 'google_colab', 'stored_token'\n",
    "\n",
    "if use_selection == 'user_prompt':\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "elif use_selection == 'google_colab':\n",
    "    from google.colab import userdata\n",
    "elif use_selection == 'stored_token':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color:#ffffff; \n",
    "          font-size:30px; \n",
    "          font-style:italic; \n",
    "          text-align:left;\n",
    "          font-family: 'Lucida Bright';\n",
    "          background:#4686C8;\">\n",
    "  \t      &nbsp; Load the HuggingFace Pretrained Model\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"\n",
    "          font-size:16px; \n",
    "          text-align:left;\n",
    "          font-family: 'Cambria';\">\n",
    "    \n",
    "Select the model and languages to use. the model name with the 'en-fr' denotes translation from english to french. Make this selection be setting the 'model_name' variable below. You can use other HuggingFace models with different languages.<br>\n",
    "Some specific details:<br>\n",
    "- tokenizer: This is used to preprocess the input text so that it is in a format usable by the model. This includes splitting the text in to words, creating a vocabulary, and representing the words as integer ID values.\n",
    "- model: This is the HuggingFace pretrained model which is the actual LLM. Since it is already pretrained, training the model is not needed saving a lot of time. The <b>TF</b>AutoModelForSeq2SeqLM object is used which loads the pretrained model in Tensorflow format so that TensorFlow/Keras methods can be used. If using PyTorch, use AutoModelForSeq2SeqLM object. Normally, the model name and the dataset name can be independently specified, but here the model and dataset names are in one entity.\n",
    "- A list of language translation models can be found here: https://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models<br>\n",
    "  On the left side of that web site, look for directories like 'en-fr' which is english to french. The model name would then be 'Helsinki-NLP/opus-mt-en-fr'. Just substitute 'en-fr' with another lanuage pair.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pcm7VYRtB9F_",
    "outputId": "fb5a68e7-5882-494a-a6d6-cf752c6716c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 15:36:25.000608: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-27 15:36:25.002374: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-27 15:36:25.007707: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-27 15:36:25.023615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751063785.051182  380872 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751063785.060202  380872 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751063785.083785  380872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751063785.083812  380872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751063785.083822  380872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751063785.083831  380872 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-27 15:36:25.091017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-27 15:36:28.333210: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name1 = 'Helsinki-NLP/opus-mt-en-nl'    # English to Dutch (Netherlands)\n",
    "model_name2 = 'Helsinki-NLP/opus-mt-en-fr'    # English to French\n",
    "model_name3 = 'Helsinki-NLP/opus-mt-en-es'    # English to Spanish\n",
    "model_name4 = 'Helsinki-NLP/opus-mt-en-tl'    # English to Tagalog (Phillipines)\n",
    "model_name = model_name2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color:#ffffff; \n",
    "          font-size:30px; \n",
    "          font-style:italic; \n",
    "          text-align:left;\n",
    "          font-family: 'Lucida Bright';\n",
    "          background:#4686C8;\">\n",
    "  \t      &nbsp; Preprocess the Input Text\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"\n",
    "          font-size:16px; \n",
    "          text-align:left;\n",
    "          font-family: 'Cambria';\">\n",
    "    \n",
    "A list of input sentences is created which will later be language translated. The tokenizer object is used to convert the input text to a format the model can use.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_M9lnnWBVt7",
    "outputId": "6d7fc13d-23ea-4b23-b1f1-13bbc1f719f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgeb/georgeb/AI-ML/my_language_translation/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4073: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "texts.append(\"Hello my friends! How are you doing today?\")\n",
    "texts.append('what is your name?')\n",
    "texts.append('what did you have for dinner yesterday?')\n",
    "texts.append('I plan to travel to Australia next summer.')\n",
    "texts.append('My name is Steve. I work at Apple Computers as a marketing manager in the engineering department.')\n",
    "\n",
    "tokenized_text = tokenizer.prepare_seq2seq_batch(texts, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color:#ffffff; \n",
    "          font-size:30px; \n",
    "          font-style:italic; \n",
    "          text-align:left;\n",
    "          font-family: 'Lucida Bright';\n",
    "          background:#4686C8;\">\n",
    "  \t      &nbsp; Perform the Language Translation\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"\n",
    "          font-size:16px; \n",
    "          text-align:left;\n",
    "          font-family: 'Cambria';\">\n",
    "    \n",
    "The generate() function will perform the actual language translation. The translated text is then printed out.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45ypTLRKCt_l",
    "outputId": "7ae44b87-6242-4c68-babf-f4f6f2ac78e9"
   },
   "outputs": [],
   "source": [
    "translation = model.generate(**tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_XsnrQ-Tm2C",
    "outputId": "bab30d0c-c22e-4d36-f85b-6032fc117383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:        Hello my friends! How are you doing today?\n",
      "translation: Bonjour mes amis, comment allez-vous aujourd'hui ?\n",
      "\n",
      "text:        what is your name?\n",
      "translation: Quel est votre nom ?\n",
      "\n",
      "text:        what did you have for dinner yesterday?\n",
      "translation: Qu'avez-vous mangé hier ?\n",
      "\n",
      "text:        I plan to travel to Australia next summer.\n",
      "translation: Je compte voyager en Australie l'été prochain.\n",
      "\n",
      "text:        My name is Steve. I work at Apple Computers as a marketing manager in the engineering department.\n",
      "translation: Je m'appelle Steve. Je travaille chez Apple Computers comme directeur marketing dans le département d'ingénierie.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translated_texts = tokenizer.batch_decode(translation, skip_special_tokens=True)\n",
    "\n",
    "for idx in range(0, len(texts)):\n",
    "    print(f'text:        {texts[idx]}')\n",
    "    print(f'translation: {translated_texts[idx]}')\n",
    "    print(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xx5bxj-ODaO2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
